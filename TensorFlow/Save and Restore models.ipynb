{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test)= keras.datasets.mnist.load_data()\n",
    "\n",
    "X_train= X_train[:1000].reshape(-1, 28*28) / 255.0\n",
    "X_test= X_test[:1000].reshape(-1, 28*28) / 255.0\n",
    "\n",
    "y_train= y_train[:1000]\n",
    "y_test= y_test[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x27f3751e9e8>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X_train[24].reshape((28,28)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model= keras.Sequential([\n",
    "        keras.layers.Dense(512, activation= tf.nn.relu, input_shape= (28*28, )),\n",
    "        keras.layers.Dropout(0.2),\n",
    "        keras.layers.Dense(10, activation= tf.nn.softmax)\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer= tf.train.AdamOptimizer(),\n",
    "                 loss= tf.keras.losses.sparse_categorical_crossentropy,\n",
    "                 metrics= ['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_2 (Dense)              (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 407,050\n",
      "Trainable params: 407,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model= create_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save checkpoint during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "checkpoint_path= 'training_1/cp.ckpt'\n",
    "checkpoing_dir= os.path.dirname(checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checkpoing callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_callback= tf.keras.callbacks.ModelCheckpoint(checkpoint_path,\n",
    "                                               save_weights_only= True,\n",
    "                                               verbose= 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model= create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples, validate on 1000 samples\n",
      "Epoch 1/10\n",
      " 864/1000 [========================>.....] - ETA: 0s - loss: 1.2551 - acc: 0.6343\n",
      "Epoch 00001: saving model to training_1/cp.ckpt\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 1.1649 - acc: 0.6630 - val_loss: 0.7176 - val_acc: 0.7860\n",
      "Epoch 2/10\n",
      " 992/1000 [============================>.] - ETA: 0s - loss: 0.4245 - acc: 0.8841\n",
      "Epoch 00002: saving model to training_1/cp.ckpt\n",
      "1000/1000 [==============================] - 0s 463us/step - loss: 0.4242 - acc: 0.8840 - val_loss: 0.5058 - val_acc: 0.8430\n",
      "Epoch 3/10\n",
      " 800/1000 [=======================>......] - ETA: 0s - loss: 0.2931 - acc: 0.9250\n",
      "Epoch 00003: saving model to training_1/cp.ckpt\n",
      "1000/1000 [==============================] - 0s 482us/step - loss: 0.2822 - acc: 0.9250 - val_loss: 0.4781 - val_acc: 0.8430\n",
      "Epoch 4/10\n",
      " 992/1000 [============================>.] - ETA: 0s - loss: 0.2101 - acc: 0.9496\n",
      "Epoch 00004: saving model to training_1/cp.ckpt\n",
      "1000/1000 [==============================] - 0s 465us/step - loss: 0.2095 - acc: 0.9500 - val_loss: 0.4409 - val_acc: 0.8640\n",
      "Epoch 5/10\n",
      " 960/1000 [===========================>..] - ETA: 0s - loss: 0.1484 - acc: 0.9656\n",
      "Epoch 00005: saving model to training_1/cp.ckpt\n",
      "1000/1000 [==============================] - 0s 495us/step - loss: 0.1501 - acc: 0.9640 - val_loss: 0.4523 - val_acc: 0.8530\n",
      "Epoch 6/10\n",
      " 992/1000 [============================>.] - ETA: 0s - loss: 0.1265 - acc: 0.9728\n",
      "Epoch 00006: saving model to training_1/cp.ckpt\n",
      "1000/1000 [==============================] - 0s 500us/step - loss: 0.1257 - acc: 0.9730 - val_loss: 0.4591 - val_acc: 0.8530\n",
      "Epoch 7/10\n",
      " 960/1000 [===========================>..] - ETA: 0s - loss: 0.0908 - acc: 0.9833\n",
      "Epoch 00007: saving model to training_1/cp.ckpt\n",
      "1000/1000 [==============================] - 1s 561us/step - loss: 0.0914 - acc: 0.9820 - val_loss: 0.4066 - val_acc: 0.8670\n",
      "Epoch 8/10\n",
      " 960/1000 [===========================>..] - ETA: 0s - loss: 0.0641 - acc: 0.9927\n",
      "Epoch 00008: saving model to training_1/cp.ckpt\n",
      "1000/1000 [==============================] - 0s 467us/step - loss: 0.0629 - acc: 0.9930 - val_loss: 0.4133 - val_acc: 0.8720\n",
      "Epoch 9/10\n",
      " 768/1000 [======================>.......] - ETA: 0s - loss: 0.0482 - acc: 0.9987\n",
      "Epoch 00009: saving model to training_1/cp.ckpt\n",
      "1000/1000 [==============================] - 0s 461us/step - loss: 0.0480 - acc: 0.9980 - val_loss: 0.4152 - val_acc: 0.8660\n",
      "Epoch 10/10\n",
      " 768/1000 [======================>.......] - ETA: 0s - loss: 0.0382 - acc: 0.9974\n",
      "Epoch 00010: saving model to training_1/cp.ckpt\n",
      "1000/1000 [==============================] - 1s 507us/step - loss: 0.0410 - acc: 0.9960 - val_loss: 0.4131 - val_acc: 0.8750\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x27f3795edd8>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train,\n",
    "         epochs= 10,\n",
    "         validation_data= (X_test, y_test),\n",
    "         callbacks= [cp_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C has no label.\n",
      " Volume Serial Number is 0E0C-78C5\n",
      "\n",
      " Directory of C:\\Users\\10650637\\Music\\ML\\Journey2AI\\TensorFlow\\training_1\n",
      "\n",
      "27-11-2018  05:47 PM    <DIR>          .\n",
      "27-11-2018  05:47 PM    <DIR>          ..\n",
      "27-11-2018  05:47 PM                71 checkpoint\n",
      "27-11-2018  05:47 PM         4,889,491 cp.ckpt.data-00000-of-00001\n",
      "27-11-2018  05:47 PM             1,439 cp.ckpt.index\n",
      "               3 File(s)      4,891,001 bytes\n",
      "               2 Dir(s)  160,019,034,112 bytes free\n"
     ]
    }
   ],
   "source": [
    "! dir {checkpoing_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Restoring on untrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model= create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 216us/step\n",
      "2.317196342468262 12.8\n"
     ]
    }
   ],
   "source": [
    "loss, acc= new_model.evaluate(X_test, y_test)\n",
    "print(loss, acc*100.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.checkpointable.util.CheckpointLoadStatus at 0x27f39bb0390>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model.load_weights(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 78us/step\n",
      "0.4130855145454407 87.5\n"
     ]
    }
   ],
   "source": [
    "loss, acc= new_model.evaluate(X_test, y_test)\n",
    "print(loss, acc*100.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoing on every 5 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path= 'training_2/cp-{epoch:04d}.ckpt'\n",
    "checkpoing_dir= os.path.dirname(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_callback= keras.callbacks.ModelCheckpoint(checkpoint_path,\n",
    "                                            verbose= 1,\n",
    "                                            save_weights_only= True,\n",
    "                                            period= 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model= create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00005: saving model to training_2/cp-0005.ckpt\n",
      "\n",
      "Epoch 00010: saving model to training_2/cp-0010.ckpt\n",
      "\n",
      "Epoch 00015: saving model to training_2/cp-0015.ckpt\n",
      "\n",
      "Epoch 00020: saving model to training_2/cp-0020.ckpt\n",
      "\n",
      "Epoch 00025: saving model to training_2/cp-0025.ckpt\n",
      "\n",
      "Epoch 00030: saving model to training_2/cp-0030.ckpt\n",
      "\n",
      "Epoch 00035: saving model to training_2/cp-0035.ckpt\n",
      "\n",
      "Epoch 00040: saving model to training_2/cp-0040.ckpt\n",
      "\n",
      "Epoch 00045: saving model to training_2/cp-0045.ckpt\n",
      "\n",
      "Epoch 00050: saving model to training_2/cp-0050.ckpt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x27f39c6c550>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train,\n",
    "         epochs= 50,\n",
    "         callbacks= [cp_callback],\n",
    "         validation_data= (X_test, y_test),\n",
    "         verbose= 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C has no label.\n",
      " Volume Serial Number is 0E0C-78C5\n",
      "\n",
      " Directory of C:\\Users\\10650637\\Music\\ML\\Journey2AI\\TensorFlow\\training_2\n",
      "\n",
      "27-11-2018  05:56 PM    <DIR>          .\n",
      "27-11-2018  05:56 PM    <DIR>          ..\n",
      "27-11-2018  05:56 PM                81 checkpoint\n",
      "27-11-2018  05:56 PM         4,889,514 cp-0005.ckpt.data-00000-of-00001\n",
      "27-11-2018  05:56 PM             1,439 cp-0005.ckpt.index\n",
      "27-11-2018  05:56 PM         4,889,514 cp-0010.ckpt.data-00000-of-00001\n",
      "27-11-2018  05:56 PM             1,439 cp-0010.ckpt.index\n",
      "27-11-2018  05:56 PM         4,889,514 cp-0015.ckpt.data-00000-of-00001\n",
      "27-11-2018  05:56 PM             1,439 cp-0015.ckpt.index\n",
      "27-11-2018  05:56 PM         4,889,514 cp-0020.ckpt.data-00000-of-00001\n",
      "27-11-2018  05:56 PM             1,439 cp-0020.ckpt.index\n",
      "27-11-2018  05:56 PM         4,889,514 cp-0025.ckpt.data-00000-of-00001\n",
      "27-11-2018  05:56 PM             1,439 cp-0025.ckpt.index\n",
      "27-11-2018  05:56 PM         4,889,514 cp-0030.ckpt.data-00000-of-00001\n",
      "27-11-2018  05:56 PM             1,439 cp-0030.ckpt.index\n",
      "27-11-2018  05:56 PM         4,889,514 cp-0035.ckpt.data-00000-of-00001\n",
      "27-11-2018  05:56 PM             1,439 cp-0035.ckpt.index\n",
      "27-11-2018  05:56 PM         4,889,514 cp-0040.ckpt.data-00000-of-00001\n",
      "27-11-2018  05:56 PM             1,439 cp-0040.ckpt.index\n",
      "27-11-2018  05:56 PM         4,889,514 cp-0045.ckpt.data-00000-of-00001\n",
      "27-11-2018  05:56 PM             1,439 cp-0045.ckpt.index\n",
      "27-11-2018  05:56 PM         4,889,514 cp-0050.ckpt.data-00000-of-00001\n",
      "27-11-2018  05:56 PM             1,439 cp-0050.ckpt.index\n",
      "              21 File(s)     48,909,611 bytes\n",
      "               2 Dir(s)  159,968,919,552 bytes free\n"
     ]
    }
   ],
   "source": [
    "! dir {checkpoing_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting the latest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'training_2\\\\cp-0050.ckpt'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latest= tf.train.latest_checkpoint(checkpoing_dir)\n",
    "latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 162us/step\n",
      "0.46666604006290435 87.7\n"
     ]
    }
   ],
   "source": [
    "model= create_model()\n",
    "model.load_weights(latest)\n",
    "loss, acc= model.evaluate(X_test, y_test)\n",
    "print(loss, acc*100.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manually save the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 167us/step\n",
      "0.46666604006290435 87.7\n"
     ]
    }
   ],
   "source": [
    "#Saving\n",
    "model.save_weights('./checkpoints/shiv_checkpoint')\n",
    "\n",
    "#Restoring\n",
    "model= create_model()\n",
    "model.load_weights('./checkpoints/shiv_checkpoint')\n",
    "\n",
    "loss, acc= model.evaluate(X_test, y_test)\n",
    "print(loss, acc* 100.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the entire model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model= create_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Need to use keras.optimizer to restore optimizer state from HDF5 file\n",
    "model.compile(optimizer= keras.optimizers.Adam(),\n",
    "             loss= keras.losses.sparse_categorical_crossentropy,\n",
    "             metrics= ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples, validate on 1000 samples\n",
      "Epoch 1/5\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 1.1548 - acc: 0.6720 - val_loss: 0.7076 - val_acc: 0.7730\n",
      "Epoch 2/5\n",
      "1000/1000 [==============================] - 0s 495us/step - loss: 0.4204 - acc: 0.8880 - val_loss: 0.5120 - val_acc: 0.8320\n",
      "Epoch 3/5\n",
      "1000/1000 [==============================] - 0s 461us/step - loss: 0.2865 - acc: 0.9300 - val_loss: 0.4534 - val_acc: 0.8560\n",
      "Epoch 4/5\n",
      "1000/1000 [==============================] - 0s 464us/step - loss: 0.2085 - acc: 0.9500 - val_loss: 0.4301 - val_acc: 0.8680\n",
      "Epoch 5/5\n",
      "1000/1000 [==============================] - 0s 487us/step - loss: 0.1646 - acc: 0.9610 - val_loss: 0.4217 - val_acc: 0.8700\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x27f3939ee48>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train,\n",
    "         validation_data= (X_test, y_test),\n",
    "         epochs= 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('shiv_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_16 (Dense)             (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 407,050\n",
      "Trainable params: 407,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "new_model= keras.models.load_model('shiv_model.h5')\n",
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 0s 85us/step\n",
      "0.42165456581115723 87.0\n"
     ]
    }
   ],
   "source": [
    "loss, acc= model.evaluate(X_test, y_test)\n",
    "print(loss, acc* 100.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To be continued...."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
